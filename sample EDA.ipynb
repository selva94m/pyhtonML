{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the Exploratory Analysis of Titanic Dataset\n",
    "# and generate various reports and plots for analysing the Data.\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandasql as pdsql\n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "matplotlib.pyplot.switch_backend('agg')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Identiy catagorial values based on some treshould.\n",
    "# Here I set the treshould as .25\n",
    "treshold_for_category = 0.25\n",
    "\n",
    "# Replace the String values in Catagory attribute to an integer.\n",
    "replace_string_catagories = {\"Sex\": {\"male\": 0, \"female\": 1},\n",
    "                             \"Embarked\": {\"S\": 1, \"C\": 2, \"Q\": 3}\n",
    "                             }\n",
    "\n",
    "\n",
    "# Mention the name of the columns that you are intrested.\n",
    "# Ensure the plotting applis only for numerical vlaues,\n",
    "# since we get min/max for ploting.\n",
    "intrested_columns = ['Fare']\n",
    "\n",
    "# If there are many unique values then the plot will have plots\n",
    "# overlapped with values. To avoid it, we can set the attributes\n",
    "# that do not need to display the unique values.\n",
    "do_notShow_unique_values_in_plot = ['Fare']\n",
    "\n",
    "\n",
    "def data_analysis(datasetName, df, targetAttribute, outDirectory):\n",
    "\n",
    "    \"\"\"\n",
    "    Function will take the dataset and perform some analysis about the data\n",
    "    Parameters\n",
    "    ----------\n",
    "    datasetName: Name of the dataset.\n",
    "    df: DataFrame for Test data\n",
    "    targetAttribute: Target Attribute\n",
    "    outDirectory: directory where the reports and plot images generated\n",
    "    \"\"\"\n",
    "\n",
    "    file = \"1_initial_data_analysis.txt\"\n",
    "\n",
    "    print(\"\\n\\nWritting the analysis report to - \", outDirectory)\n",
    "\n",
    "    print(\"\\n\\nWriting the overview of data to \" + outDirectory + \"/\" + file)\n",
    "\n",
    "    # Create a directory to write the output and report.\n",
    "    os.system(\"mkdir \" + outDirectory)\n",
    "\n",
    "    # Output will be written to initial_data_analysis.txt\n",
    "    report = open(outDirectory + \"/\" + file, 'w+')\n",
    "\n",
    "    # Write the dataset name, date the report generted\n",
    "    # into initial_data_analysis.txt\n",
    "    print('\\n=============================================' +\n",
    "          '===============================================',\n",
    "          file=report)\n",
    "    print('Dataset : ', datasetName, file=report)\n",
    "    now = str(datetime.datetime.now())\n",
    "    print('Date : ', now, file=report)\n",
    "    print('\\n=============================================' +\n",
    "          '===============================================\\n',\n",
    "          file=report)\n",
    "\n",
    "    # Write the attribute name/count, row count of the dataset to report file\n",
    "    instance_count, attr_count = df.shape\n",
    "    attr_name = list(df)\n",
    "    print('Instance Count : ', instance_count, file=report)\n",
    "    print('Attribute count (X,y) : ', attr_count, file=report)\n",
    "    print('Attribute Names (X,y) : ', attr_name, file=report)\n",
    "\n",
    "    # Identify the attributes that have numeric and String values\n",
    "    # and write it to initial_data_analysis.txt\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    print('\\n', file=report)\n",
    "    print('Attribute with Numeric Type : ', numeric_cols, file=report)\n",
    "    item_list = list(df)\n",
    "    item_list = [e for e in item_list if e not in numeric_cols]\n",
    "    print('Attribute with String Type : ', item_list, file=report)\n",
    "    print('\\n=============================================' +\n",
    "          '===============================================\\n',\n",
    "          file=report)\n",
    "\n",
    "    # Write sample data to initial_data_analysis.txt\n",
    "    print('Sample data : ', file=report)\n",
    "    print(df.head(5), file=report)\n",
    "    print('\\n=============================================' +\n",
    "          '===============================================\\n',\n",
    "          file=report)\n",
    "\n",
    "    # describe data to initial_data_analysis.txt\n",
    "    des = df.describe()\n",
    "    print('Describe Attributes : ', file=report)\n",
    "    print('\\n', file=report)\n",
    "    print(des, file=report)\n",
    "    print('\\n=============================================' +\n",
    "          '===============================================\\n',\n",
    "          file=report)\n",
    "\n",
    "    # Write attributes that have missing values to initial_data_analysis.txt\n",
    "    missCol = pd.isnull(df).any()\n",
    "    print('Attributes that have Missing Values : ', file=report)\n",
    "    print('\\n', file=report)\n",
    "    print(missCol, file=report)\n",
    "    print('\\n=============================================' +\n",
    "          '===============================================\\n',\n",
    "          file=report)\n",
    "\n",
    "    # Write sum of attributes that have missing values\n",
    "    # to initial_data_analysis.txt\n",
    "    sumCol = pd.isnull(df).sum()\n",
    "    print('Sum of Missing Values for each attributes : ', file=report)\n",
    "    print('\\n', file=report)\n",
    "    print(sumCol, file=report)\n",
    "    print('\\n=============================================' +\n",
    "          '===============================================\\n',\n",
    "          file=report)\n",
    "\n",
    "    mostly_cat = {}\n",
    "    category_column_list = list()\n",
    "    for var in df.columns:\n",
    "            mostly_cat[var] = 1.*df[var].nunique()/df[var].count() < \\\n",
    "                                treshold_for_category\n",
    "    for key, value in mostly_cat.items():\n",
    "        if value:\n",
    "            category_column_list.append(key)\n",
    "    print('Most likely cataegorial values : ', file=report)\n",
    "    print(category_column_list, file=report)\n",
    "\n",
    "    non_category_column_list = [e for e in attr_name\n",
    "                                if e not in\n",
    "                                category_column_list]\n",
    "    print('\\n\\nMost likely **Non cataegorial values : ', file=report)\n",
    "    print(non_category_column_list, file=report)\n",
    "\n",
    "    print('\\n=============================================' +\n",
    "          '===============================================\\n',\n",
    "          file=report)\n",
    "\n",
    "    # display the unique values for catagorial attributes\n",
    "    for col in category_column_list:\n",
    "        print('Unique values for cataegorial column : ', col, file=report)\n",
    "        print(df[col].unique(), file=report)\n",
    "        print('\\n ', file=report)\n",
    "\n",
    "    report.close()\n",
    "\n",
    "    return category_column_list\n",
    "\n",
    "\n",
    "def updateMissingOrStringCatagorialValues(df):\n",
    "    print(\"\\n\\nUpdating the missing Values and Replace\" +\n",
    "          \" the catagorial string values to integer\")\n",
    "\n",
    "    # Fill the missing Age values to median\n",
    "    df['Age'].fillna(df['Age'].median(), inplace=True)\n",
    "\n",
    "    # Fill the missing values for Cabin as missing\n",
    "    df['Cabin'].fillna('Missing', inplace=True)\n",
    "\n",
    "    # Fill the missing values for Embarked as  S = Southampton\n",
    "    # since it has the highest no# of people embarked.\n",
    "    df['Embarked'].fillna('S', inplace=True)\n",
    "\n",
    "    # Replace the String values in Catagory attribute to an integer.\n",
    "    # \"Sex\": {\"male\": 0, \"female\": 1}\n",
    "    # \"Embarked\": {\"S\": 1, \"C\": 2, \"Q\": 3}\n",
    "    df.replace(replace_string_catagories, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def plot(df, category_column_list, targetAttribute, outDirectory):\n",
    "\n",
    "    instance_count, attr_count = df.shape\n",
    "    sumCol = pd.isnull(df).sum()\n",
    "\n",
    "    for rows in range(1, 6):\n",
    "        size = rows * 5\n",
    "        if size >= attr_count:\n",
    "            break\n",
    "\n",
    "    # Do a ploting of Histogram to get the count\n",
    "    pl.rcParams.update({'font.size': 18})\n",
    "    df.hist()\n",
    "    fig = matplotlib.pyplot.gcf()\n",
    "    fig.set_size_inches(30, 15)\n",
    "    # fig.savefig('test2png.png', dpi=100)\n",
    "    fig.savefig(outDirectory + \"/2_Histogram_plot.png\", dpi=100)\n",
    "    pl.rcParams.update({'font.size': 10})\n",
    "\n",
    "    # Create the box plot for categorial values\n",
    "    pl.rcParams.update({'font.size': 18})\n",
    "    df.plot(kind='box', subplots=True, layout=(rows, 5), sharex=False)\n",
    "    fig = matplotlib.pyplot.gcf()\n",
    "    fig.set_size_inches(30, 15)\n",
    "    fig.savefig(outDirectory + \"/3_Box_plot.png\", tight_layout=True)\n",
    "    pl.rcParams.update({'font.size': 10})\n",
    "\n",
    "    for col in category_column_list:\n",
    "        bar_density_plot(outDirectory, df, col, replace_string_catagories,\n",
    "                         do_notShow_unique_values_in_plot, sumCol,\n",
    "                         category_column_list, treshold_for_category)\n",
    "\n",
    "    for col in intrested_columns:\n",
    "        bar_density_plot(outDirectory, df, col, replace_string_catagories,\n",
    "                         do_notShow_unique_values_in_plot, sumCol,\n",
    "                         category_column_list, treshold_for_category)\n",
    "\n",
    "    for col in category_column_list:\n",
    "        histogram(df, col, targetAttribute, df[col].nunique(), col,\n",
    "                  'Frequency', 'Distribution of ' + col + ' with respect to ' +\n",
    "                  targetAttribute, outDirectory + \"/5_\" + col + \"_GroupBy_\" +\n",
    "                  targetAttribute + \"_Histogram_plot.png\")\n",
    "\n",
    "    # Plot the pairwise plotting with respect to Target attribute\n",
    "    pl.rcParams.update({'font.size': 18})\n",
    "    df1 = pd.DataFrame(df, columns=category_column_list)\n",
    "    plt.figure()\n",
    "    sns.pairplot(data=df1, hue=targetAttribute)\n",
    "    plt.savefig(outDirectory + \"/6_pairwise_plot.png\")\n",
    "    pl.rcParams.update({'font.size': 10})\n",
    "\n",
    "\n",
    "def generalizeAttribute(df):\n",
    "\n",
    "    df['Age_group'] = pd.cut(df['Age'], [0, 10, 20, 30,\n",
    "                                         40, 50, 60, 70,\n",
    "                                         80, 90],\n",
    "                             labels=['0-10', '10-20',\n",
    "                                     '20-30', '30-40',\n",
    "                                     '40-50', '50-60',\n",
    "                                     '60-70', '70-80',\n",
    "                                     '80-90'])\n",
    "\n",
    "    cabin_dict = {'A': 'A_Cabin', 'B': 'B_Cabin', 'C': 'C_Cabin',\n",
    "                  'D': 'D_Cabin', 'E': 'E_Cabin', 'F': 'F_Cabin',\n",
    "                  'G': 'G_Cabin', 'T': 'T_Cabin', 'M': 'Missing'}\n",
    "\n",
    "    df['Cabin_group'] = df.Cabin.str[:1].apply(lambda val: cabin_dict[val])\n",
    "\n",
    "    # Fare starts from 0.0 so we need to put -1 when we cut it\n",
    "    df['Fare_group'] = pd.cut(df['Fare'], [-1, 50, 100, 150, 200,\n",
    "                              250, 300, 350, 400,\n",
    "                              450, 500, 550],\n",
    "                              labels=['0-50', '50-100', '100-150',\n",
    "                                      '150-200', '200-250',\n",
    "                                      '250-300', '300-350',\n",
    "                                      '350-400', '400-450',\n",
    "                                      '450-500', '500-550'])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def generalizedAttributePlot(df, targetAttribute, outDirectory):\n",
    "\n",
    "    generalizedAttributeList = ['Age_group', 'Cabin_group', 'Fare_group']\n",
    "\n",
    "    for col in generalizedAttributeList:\n",
    "        histogram(df[col], None, None,\n",
    "                  df[col].nunique(), col,\n",
    "                  'Frequency', 'Distribution of ' + col,\n",
    "                  outDirectory + \"/7_Count_\" + col + \"_GroupedValues_plot.png\")\n",
    "\n",
    "    for col in generalizedAttributeList:\n",
    "        histogram(df, col, targetAttribute, df[col].nunique(), col,\n",
    "                  'Frequency', 'Distribution of ' + col + ' with respect to ' +\n",
    "                  targetAttribute, outDirectory + \"/8_\" + col + \"_GroupBy_\" +\n",
    "                  targetAttribute + \"_Histogram_plot.png\")\n",
    "\n",
    "\n",
    "def groupByData(df, category_column_list, targetAttribute, outDirectory):\n",
    "    file = \"9_GroupBy_Attribute_based_on_Target.txt\"\n",
    "\n",
    "    # Output will be written to initial_data_analysis.txt\n",
    "    report = open(outDirectory + \"/\" + file, 'w+')\n",
    "\n",
    "    print(\"\\n\\nWriting Group By Attribute data based on Target Attribute \" +\n",
    "          outDirectory + \"/\" + file)\n",
    "\n",
    "    attributes_for_sql_analysis = list()\n",
    "    attributes_for_sql_analysis.extend(category_column_list)\n",
    "    attributes_for_sql_analysis.extend(intrested_columns)\n",
    "    attributes_for_sql_analysis.append('Age_group')\n",
    "    attributes_for_sql_analysis.append('Cabin_group')\n",
    "    attributes_for_sql_analysis.append('Fare_group')\n",
    "    attributes_for_sql_analysis.remove(targetAttribute)\n",
    "\n",
    "    print('Groupby on attribute values with respect to target attribute : ',\n",
    "          file=report)\n",
    "    print('\\n', file=report)\n",
    "\n",
    "    for col in attributes_for_sql_analysis:\n",
    "        # pysql = lambda q: pdsql.sqldf(q, locals())\n",
    "        queryStr = \"select \" + col + \", count(\" + col + \") as Count, \" + \\\n",
    "                    targetAttribute + \" from df group by \" + col + \\\n",
    "                    \", \" + targetAttribute + \" order by \" + col + \";\"\n",
    "\n",
    "        df1 = pdsql.sqldf(queryStr.lower(), locals())\n",
    "\n",
    "        # Do a groupby on attribute values with respect to target attribute\n",
    "        print('Group by on Attribute : ' + col, file=report)\n",
    "\n",
    "        if col in replace_string_catagories:\n",
    "            print('Dictionary Mapping : ' +\n",
    "                  str(replace_string_catagories[col]) + '\\n', file=report)\n",
    "\n",
    "        print(df1.to_string(), file=report)\n",
    "        print('\\n ', file=report)\n",
    "        print('\\n ', file=report)\n",
    "\n",
    "    print('\\n=============================================' +\n",
    "          '===============================================',\n",
    "          file=report)\n",
    "    print('\\n ', file=report)\n",
    "\n",
    "    report.close()\n",
    "\n",
    "\n",
    "def cross_attribute_analysis(df, intrestedCrossAttributeList, targetAttribute,\n",
    "                             doNormalise, outDirectory, inpFile):\n",
    "\n",
    "    print(\"\\n\\nWritting the Cross Attribute Analysis report to - \",\n",
    "          outDirectory)\n",
    "\n",
    "    print(\"\\n\\nWriting the overview of data to \" + outDirectory +\n",
    "          \"/\" + inpFile)\n",
    "\n",
    "    # Create a directory to write the output and report.\n",
    "    os.system(\"mkdir \" + outDirectory)\n",
    "\n",
    "    # Output will be written to initial_data_analysis.txt\n",
    "    report = open(outDirectory + \"/\" + inpFile, 'w+')\n",
    "\n",
    "    crossAttributeList = list()\n",
    "    index = 0\n",
    "    attributeListLength = len(intrestedCrossAttributeList)\n",
    "    for i in range(0, attributeListLength):\n",
    "        counter = i + 1\n",
    "\n",
    "        for j in range(counter, attributeListLength):\n",
    "            crossAttributeList.append([])\n",
    "            crossAttributeList[index].append(intrestedCrossAttributeList[i])\n",
    "            crossAttributeList[index].append(intrestedCrossAttributeList[j])\n",
    "            index = index + 1\n",
    "\n",
    "    index = 1\n",
    "    length = len(crossAttributeList)\n",
    "    print(\"\\nTotal Cross Attribute No# - \" + str(length) + \"\\n\")\n",
    "    for item in crossAttributeList:\n",
    "        print(\"Processing the cross attribute ( \" + item[0] + \" & \" +\n",
    "              item[1] + \" ) \" + str(index) + \" out of \" + str(length))\n",
    "\n",
    "        file = outDirectory + \"/\" + \"2_CrossAttribute_\" + item[0] + \"_\" + \\\n",
    "                              item[1] + \"_Count.png\"\n",
    "\n",
    "        groupBucket = df.groupby(item)\n",
    "\n",
    "        groupCount = groupBucket[targetAttribute].count()\n",
    "\n",
    "        # Write the dataset name, date the report generted\n",
    "        # into initial_data_analysis.txt\n",
    "        print('\\n=============================================' +\n",
    "              '===============================================',\n",
    "              file=report)\n",
    "        print('Group By - ' + item[0] + \" & \" + item[1] + '\\n', file=report)\n",
    "\n",
    "        content = \"\"\n",
    "        if item[0] in replace_string_catagories:\n",
    "            content = content + 'Dictionary Mapping : ' + \\\n",
    "                      str(replace_string_catagories[item[0]]) + '\\n'\n",
    "\n",
    "        if item[1] in replace_string_catagories:\n",
    "            content = content + 'Dictionary Mapping : ' + \\\n",
    "                      str(replace_string_catagories[item[1]]) + '\\n'\n",
    "\n",
    "        print(content, file=report)\n",
    "\n",
    "        print(groupCount.to_string(), file=report)\n",
    "        print('\\n=============================================' +\n",
    "              '===============================================\\n',\n",
    "              file=report)\n",
    "        groupCountUnstack = groupCount.unstack()\n",
    "\n",
    "        pl.rcParams.update({'font.size': 18})\n",
    "\n",
    "        groupCountUnstack.plot(kind='bar', title='Grouped by - ' +\n",
    "                               item[0] + \" & \" + item[1] + \"\\n\" + content)\n",
    "        plt.xlabel(item[0])\n",
    "        plt.ylabel(\"frequency\")\n",
    "        fig = matplotlib.pyplot.gcf()\n",
    "        fig.set_size_inches(30, 15)\n",
    "        fig.savefig(file)\n",
    "        pl.rcParams.update({'font.size': 10})\n",
    "        index = index + 1\n",
    "\n",
    "    report.close()\n",
    "\n",
    "\n",
    "def cross_attribute_with_target_analysis(df, intrestedCrossAttributeList,\n",
    "                                         targetAttribute, doNormalise,\n",
    "                                         outDirectory, inpFile):\n",
    "\n",
    "    print(\"\\n\\nWritting the Cross Attribute Analysis with target report to - \",\n",
    "          outDirectory)\n",
    "\n",
    "    print(\"\\n\\nWriting the overview of data to \" +\n",
    "          outDirectory + \"/\" + inpFile)\n",
    "\n",
    "    # Create a directory to write the output and report.\n",
    "    os.system(\"mkdir \" + outDirectory)\n",
    "\n",
    "    # Output will be written to initial_data_analysis.txt\n",
    "    report = open(outDirectory + \"/\" + inpFile, 'w+')\n",
    "\n",
    "    crossAttributeList = list()\n",
    "    index = 0\n",
    "    attributeListLength = len(intrestedCrossAttributeList)\n",
    "    for i in range(0, attributeListLength):\n",
    "        counter = i + 1\n",
    "\n",
    "        for j in range(counter, attributeListLength):\n",
    "            crossAttributeList.append([])\n",
    "            crossAttributeList[index].append(intrestedCrossAttributeList[i])\n",
    "            crossAttributeList[index].append(intrestedCrossAttributeList[j])\n",
    "            index = index + 1\n",
    "\n",
    "    index = 1\n",
    "    length = len(crossAttributeList)\n",
    "    print(\"\\nTotal Cross Attribute No# - \" + str(length) + \"\\n\")\n",
    "    for item in crossAttributeList:\n",
    "        print(\"Processing the cross attribute ( \" + item[0] + \" & \" +\n",
    "              item[1] + \" & \" + targetAttribute + \" ) \" + str(index) +\n",
    "              \" out of \" + str(length))\n",
    "\n",
    "        file = outDirectory + \"/\" + \"2_CrossAttribute_\" + item[0] + \"_\" + \\\n",
    "                              item[1] + \"_\" + targetAttribute + \".png\"\n",
    "\n",
    "        item.append(targetAttribute)\n",
    "        groupBucket = df.groupby(item)\n",
    "        groupCount = groupBucket[targetAttribute].count()\n",
    "\n",
    "        # Write the dataset name, date the report generted\n",
    "        # into initial_data_analysis.txt\n",
    "        print('\\n=============================================' +\n",
    "              '===============================================',\n",
    "              file=report)\n",
    "        print('Group By - ' + item[0] + \" & \" + item[1] + \" & \" +\n",
    "              targetAttribute + '\\n', file=report)\n",
    "\n",
    "        content = \"\"\n",
    "        if item[0] in replace_string_catagories:\n",
    "            content = content + 'Dictionary Mapping : ' + \\\n",
    "                      str(replace_string_catagories[item[0]]) + '\\n'\n",
    "\n",
    "        if item[1] in replace_string_catagories:\n",
    "            content = content + 'Dictionary Mapping : ' + \\\n",
    "                      str(replace_string_catagories[item[1]]) + \\\n",
    "                      '\\n'\n",
    "\n",
    "        print(content, file=report)\n",
    "\n",
    "        print(groupCount.to_string(), file=report)\n",
    "        print('\\n=============================================' +\n",
    "              '===============================================\\n',\n",
    "              file=report)\n",
    "        groupCountUnstack = groupCount.unstack()\n",
    "        pl.rcParams.update({'font.size': 15})\n",
    "        groupCountUnstack.plot(kind='bar', title='Grouped by - ' +\n",
    "                               item[0] + \" & \" + item[1] + \" & \" +\n",
    "                               targetAttribute + \"\\n\" + content)\n",
    "        plt.xlabel(item[0] + \" & \" + item[1])\n",
    "        plt.ylabel(\"frequency\")\n",
    "        fig = matplotlib.pyplot.gcf()\n",
    "        fig.set_size_inches(30, 15)\n",
    "        fig.savefig(file)\n",
    "        pl.rcParams.update({'font.size': 10})\n",
    "        index = index + 1\n",
    "\n",
    "    report.close()\n",
    "\n",
    "\n",
    "def exploratory_analysis(datasetName, df, targetAttribute, outDirectory):\n",
    "\n",
    "    \"\"\"\n",
    "    Function will take the dataset and perform some analysis\n",
    "    and generate the reports to visualize the data and to\n",
    "    identify the outliners.\n",
    "    Parameters\n",
    "    ----------\n",
    "    datasetName: Name of the dataset.\n",
    "    df: DataFrame for Test data\n",
    "    targetAttribute: Target Attribute\n",
    "    outDirectory: directory where the reports and plot images generated\n",
    "    \"\"\"\n",
    "\n",
    "    instance_count, attr_count = df.shape\n",
    "    sumCol = pd.isnull(df).sum()\n",
    "\n",
    "    category_column_list = data_analysis(datasetName, df,\n",
    "                                         targetAttribute,\n",
    "                                         outDirectory)\n",
    "\n",
    "    df = updateMissingOrStringCatagorialValues(df)\n",
    "\n",
    "    plot(df, category_column_list, targetAttribute, outDirectory)\n",
    "\n",
    "    df = generalizeAttribute(df)\n",
    "\n",
    "    generalizedAttributePlot(df, targetAttribute, outDirectory)\n",
    "\n",
    "    groupByData(df, category_column_list, targetAttribute, outDirectory)\n",
    "\n",
    "    intrestedCrossAttributeList = ['Pclass', 'Sex', 'Age_group', 'SibSp',\n",
    "                                   'Parch', 'Fare_group', 'Cabin_group',\n",
    "                                   'Embarked']\n",
    "    doNormalise = True\n",
    "    crossDirectory = outDirectory + \"/\" + \"10_CrossAttributeAnalysis\"\n",
    "    inpFile = \"1_CrossAttribute_data_analysis.txt\"\n",
    "\n",
    "    cross_attribute_analysis(df, intrestedCrossAttributeList, targetAttribute,\n",
    "                             doNormalise, crossDirectory, inpFile)\n",
    "\n",
    "    crossDirectory = outDirectory + \"/\" + \"11_CrossAttributeWithTargetAnalysis\"\n",
    "    inpFile = \"1_CrossAttribute_Target_data_analysis.txt\"\n",
    "    print(intrestedCrossAttributeList)\n",
    "    cross_attribute_with_target_analysis(df, intrestedCrossAttributeList,\n",
    "                                         targetAttribute, doNormalise,\n",
    "                                         crossDirectory, inpFile)\n",
    "\n",
    "\n",
    "def histogram(df, col, targetAttribute, bin, xLabel, yLabel, title, file):\n",
    "    if col is not None and col == targetAttribute:\n",
    "        return\n",
    "\n",
    "    if targetAttribute is not None:\n",
    "        pl.rcParams.update({'font.size': 18})\n",
    "\n",
    "        df[col].hist(by=df[targetAttribute], bins=df[col].nunique())\n",
    "\n",
    "        title = 'Attribute Name: ' + col + \\\n",
    "                ' grouped by Target Attribute : ' + \\\n",
    "                targetAttribute\n",
    "\n",
    "        if col in replace_string_catagories:\n",
    "            title = title + \"\\n\\n Dictionary Mapping :\" + \\\n",
    "                    str(replace_string_catagories[col])\n",
    "\n",
    "        pl.suptitle(title)\n",
    "        fig = matplotlib.pyplot.gcf()\n",
    "        fig.set_size_inches(30, 15)\n",
    "        fig.savefig(file)\n",
    "        pl.rcParams.update({'font.size': 10})\n",
    "    else:\n",
    "        plt.figure()\n",
    "        f, ax = plt.subplots(figsize=(30, 15))\n",
    "        ax.hist(df, bins=bin)\n",
    "        ax.set_ylabel(yLabel)\n",
    "        ax.set_xlabel(xLabel)\n",
    "        ax.set_title(title)\n",
    "\n",
    "        for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "                     ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "            item.set_fontsize(20)\n",
    "\n",
    "        plt.savefig(file)\n",
    "\n",
    "\n",
    "def bar_density_plot(outDirectory, df, col, replace_string_catagories,\n",
    "                     do_notShow_unique_values_in_plot, sumCol,\n",
    "                     category_column_list, treshold_for_category):\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    plt.subplot(312)\n",
    "    plt.xlim(df[col].min(), df[col].max()*1.1)\n",
    "\n",
    "    ax = df[col].plot(kind='kde')\n",
    "\n",
    "    plt.subplot(313)\n",
    "    plt.xlim(df[col].min(), df[col].max()*1.1)\n",
    "    sns.boxplot(x=df[col])\n",
    "\n",
    "    title = 'Attribute Name: ' + col\n",
    "\n",
    "    title = title + '\\n\\nNo of missing values : ' + str(sumCol[col])\n",
    "\n",
    "    if col in category_column_list:\n",
    "        title = title + \"\\n\\n Mostly categorial value based on treshold \" + \\\n",
    "                str(treshold_for_category) + \" ,nrows: \" + \\\n",
    "                str(df[col].count()) + \" ,unique values: \" + \\\n",
    "                str(df[col].nunique())\n",
    "    else:\n",
    "        title = title + \"\\n\\n Mostly **NOT** a categorial value\" + \\\n",
    "                \" based on treshold \" + str(treshold_for_category) + \\\n",
    "                \" ,nrows: \" + str(df[col].count()) + \\\n",
    "                \" ,unique values: \" + str(df[col].nunique())\n",
    "\n",
    "    if col not in do_notShow_unique_values_in_plot:\n",
    "        title = title + '\\n\\nUnique values : ' + \\\n",
    "                np.array_str(np.sort(df[col].unique()))\n",
    "\n",
    "    if col in replace_string_catagories:\n",
    "        title = title + \"\\n\\n Dictionary Mapping :\" + \\\n",
    "                str(replace_string_catagories[col])\n",
    "\n",
    "    plt.suptitle(title)\n",
    "\n",
    "    plt.savefig(outDirectory + \"/4_\" + col + \"_density_box_plot.png\")\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    \"\"\"\n",
    "    Function will take the dataset and perform some analysis\n",
    "    and generate the reports to visualize the data and to\n",
    "    identify the outliners.\n",
    "    Arguments Passed:\n",
    "    -----------------\n",
    "    datasetName: Name of the dataset.\n",
    "    train_file: Path of the Test data\n",
    "    targetAttribute: Target Attribute\n",
    "    outDirectory: directory where the reports and plot images generated\n",
    "    Arguments Passed:\n",
    "    -----------------\n",
    "    python ExploratoryDataAnalysis.py Titanic /opt/PythonConda/ML/titan/train.csv\n",
    "    Survived /opt/git_projects/Exploratory-Data-Analysis-with-python/output\n",
    "    \"\"\"\n",
    "\n",
    "    datasetName = sys.argv[1]\n",
    "    train_file = sys.argv[2]\n",
    "    targetAttribute = sys.argv[3]\n",
    "    outDirectory = sys.argv[4]\n",
    "\n",
    "    print(\"\\n\\nExploratory Analysis of Dataset\")\n",
    "\n",
    "    df = pd.read_csv(train_file)\n",
    "    exploratory_analysis(datasetName, df, targetAttribute, outDirectory)\n",
    "\n",
    "    print(\"\\n\\nExploratory Analysis completed \\n\\n\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
